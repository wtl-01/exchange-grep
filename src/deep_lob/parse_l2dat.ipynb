{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac05c7b6",
   "metadata": {},
   "source": [
    "## Parse the L2 CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d14a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install polars pandas plotly numpy scikit-learn torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f08cc7",
   "metadata": {},
   "source": [
    "### Process 1 CSV first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b168d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace 'path/to/your/file.csv' with the actual path to your CSV file\n",
    "# the last two columns are , [ with commas inside, so we need to handle that]\n",
    "\n",
    "file_path = 'CFE_20241006_Level2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['bids'] = df['bids'].apply(ast.literal_eval)\n",
    "df['asks'] = df['asks'].apply(ast.literal_eval)\n",
    "\n",
    "df_plr = pl.from_pandas(df)\n",
    "display(df_plr.head())\n",
    "\n",
    "#print(df_plr[\"asks\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b3f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_plr = df_plr.with_columns(\n",
    "    pl.col(\"srcTime\").str.to_datetime(format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    ")\n",
    "display(df_plr.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create expressions to extract bid and ask quantities and numbers for 10 levels\n",
    "expressions = []\n",
    "for i in range(10):\n",
    "    # Bids: quantity and number of orders\n",
    "    expressions.append(pl.col(\"bids\").list.get(i).list.get(0).alias(f\"bid_qty_{i}\"))\n",
    "    expressions.append(pl.col(\"bids\").list.get(i).list.get(1).alias(f\"bid_num_{i}\"))\n",
    "    # Asks: quantity and number of orders\n",
    "    expressions.append(pl.col(\"asks\").list.get(i).list.get(0).alias(f\"ask_qty_{i}\"))\n",
    "    expressions.append(pl.col(\"asks\").list.get(i).list.get(1).alias(f\"ask_num_{i}\"))\n",
    "\n",
    "# Add the new columns to the DataFrame and drop the original list columns\n",
    "df_plr_mod = df_plr.with_columns(expressions).drop(\"bids\", \"asks\")\n",
    "\n",
    "# Display the head of the transformed DataFrame to verify\n",
    "display(df_plr_mod.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current working directory\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bafe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Find all CSV files in the current directory matching the pattern\n",
    "csv_files = glob.glob('CFE_*_Level2.csv')\n",
    "\n",
    "# List to hold processed dataframes\n",
    "all_dfs = []\n",
    "\n",
    "# Re-create the expressions for expanding bids/asks as they were defined in the previous cell\n",
    "expressions = []\n",
    "for i in range(10):\n",
    "    # Bids: quantity and number of orders\n",
    "    expressions.append(pl.col(\"bids\").list.get(i).list.get(0).alias(f\"bid_qty_{i}\"))\n",
    "    expressions.append(pl.col(\"bids\").list.get(i).list.get(1).alias(f\"bid_num_{i}\"))\n",
    "    # Asks: quantity and number of orders\n",
    "    expressions.append(pl.col(\"asks\").list.get(i).list.get(0).alias(f\"ask_qty_{i}\"))\n",
    "    expressions.append(pl.col(\"asks\").list.get(i).list.get(1).alias(f\"ask_num_{i}\"))\n",
    "\n",
    "for file_path in csv_files:\n",
    "    print(f\"Processing {file_path}...\")\n",
    "    # Read the CSV file into a pandas DataFrame\n",
    "    temp_df_pd = pd.read_csv(file_path)\n",
    "    \n",
    "    # Apply literal_eval to parse the string representations of lists\n",
    "    temp_df_pd['bids'] = temp_df_pd['bids'].apply(ast.literal_eval)\n",
    "    temp_df_pd['asks'] = temp_df_pd['asks'].apply(ast.literal_eval)\n",
    "    \n",
    "    # Convert to Polars DataFrame\n",
    "    temp_df_pl = pl.from_pandas(temp_df_pd)\n",
    "    \n",
    "    # Process the DataFrame\n",
    "    temp_df_pl = temp_df_pl.with_columns(\n",
    "        pl.col(\"srcTime\").str.to_datetime(format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    ).with_columns(expressions).drop(\"bids\", \"asks\")\n",
    "\n",
    "    # write to processed folder\n",
    "    data_dir = \"processed_data\"\n",
    "    data_dir_full = os.path.join(os.getcwd(), data_dir)\n",
    "    processed_file_path = os.path.join(data_dir_full, os.path.basename(file_path))\n",
    "    os.makedirs(\"processed_data\", exist_ok=True)\n",
    "    temp_df_pl.write_csv(processed_file_path)\n",
    "    print(f\"Processed file saved to {processed_file_path}\")\n",
    "    \n",
    "    #all_dfs.append(temp_df_pl)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "# if all_dfs:\n",
    "#     df_combined = pl.concat(all_dfs)\n",
    "\n",
    "#     # Sort the combined DataFrame\n",
    "#     df_sorted = df_combined.sort(\"pktSeqNum\", \"msgSeqNum\")\n",
    "\n",
    "#     # Display the head of the final sorted DataFrame\n",
    "#     display(df_sorted.head())\n",
    "#     print(f\"Total rows in combined dataframe: {df_sorted.shape[0]}\")\n",
    "# else:\n",
    "#     print(\"No CSV files found to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import polars as pl\n",
    "\n",
    "# Directory where processed CSVs are stored\n",
    "data_dir = \"processed_data\"\n",
    "\n",
    "# Find all processed CSV files\n",
    "processed_csv_files = glob.glob(os.path.join(data_dir, 'CFE_*_Level2.csv'))\n",
    "\n",
    "if not processed_csv_files:\n",
    "    print(\"No processed CSV files found to concatenate.\")\n",
    "else:\n",
    "    print(f\"Found {len(processed_csv_files)} files to concatenate.\")\n",
    "    \n",
    "    # Lazily read all CSVs, which is more memory-efficient\n",
    "    lazy_dfs = [pl.scan_csv(f) for f in processed_csv_files]\n",
    "    \n",
    "    # Concatenate all lazy dataframes and then collect the result\n",
    "    df_concatenated = pl.concat(lazy_dfs, how=\"vertical\").collect()\n",
    "\n",
    "    # Sort the combined DataFrame to ensure chronological order\n",
    "    df_sorted = df_concatenated.sort(\"srcTime\", \"pktSeqNum\", \"msgSeqNum\")\n",
    "\n",
    "    df_sorted = df_sorted.with_columns(\n",
    "        pl.col(\"srcTime\").str.to_datetime(format=\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    )\n",
    "\n",
    "    # Define the output path for the Parquet file\n",
    "    output_parquet_path = os.path.join(data_dir, 'concatenated_l2_data.parquet')\n",
    "    \n",
    "    # Write the sorted DataFrame to a Parquet file\n",
    "    df_sorted.write_parquet(output_parquet_path)\n",
    "    \n",
    "    print(f\"Successfully concatenated and saved data to {output_parquet_path}\")\n",
    "    display(df_sorted.head())\n",
    "    print(f\"Total rows in concatenated dataframe: {df_sorted.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for Symbol 'VXV4' and tradingStatus 'T'\n",
    "df_filtered = df_sorted.filter(\n",
    "    (pl.col(\"Symbol\") == \"VXV4\") & (pl.col(\"tradingStatus\") == \"T\")\n",
    ")\n",
    "\n",
    "# Display the head of the filtered DataFrame\n",
    "display(df_filtered.head())\n",
    "\n",
    "# save the file to a new parquet\n",
    "filtered_parquet_path = os.path.join(data_dir, 'filtered_vxv4_trading.parquet')\n",
    "df_filtered.write_parquet(filtered_parquet_path)\n",
    "print(f\"Filtered data saved to {filtered_parquet_path}\")\n",
    "\n",
    "# Print the shape of the filtered dataframe to see how many rows match\n",
    "print(f\"Shape of the filtered dataframe: {df_filtered.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
